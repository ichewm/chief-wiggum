#!/usr/bin/env bash
# Chief Wiggum - Worker orchestration runner
#
# Orchestrates workers for executing software engineering tasks. Uses the
# scheduler module for unified worker tracking, priority workers, and status display.
set -euo pipefail

WIGGUM_HOME="${WIGGUM_HOME:-$HOME/.claude/chief-wiggum}"
PROJECT_DIR="$(pwd)"
RALPH_DIR="${RALPH_DIR:-$PROJECT_DIR/.ralph}"

# Source shared libraries
source "$WIGGUM_HOME/lib/core/exit-codes.sh"
source "$WIGGUM_HOME/lib/core/defaults.sh"
source "$WIGGUM_HOME/lib/core/logger.sh"
source "$WIGGUM_HOME/lib/core/file-lock.sh"
source "$WIGGUM_HOME/lib/utils/audit-logger.sh"
source "$WIGGUM_HOME/lib/utils/activity-log.sh"
source "$WIGGUM_HOME/lib/worker/worker-lifecycle.sh"
source "$WIGGUM_HOME/lib/worker/git-state.sh"
source "$WIGGUM_HOME/lib/claude/usage-tracker.sh"
source "$WIGGUM_HOME/lib/git/worktree-helpers.sh"
source "$WIGGUM_HOME/lib/tasks/task-parser.sh"
source "$WIGGUM_HOME/lib/tasks/plan-parser.sh"
source "$WIGGUM_HOME/lib/tasks/conflict-detection.sh"

# Source scheduler module (unified worker management)
source "$WIGGUM_HOME/lib/scheduler/scheduler.sh"

# Default configuration
MAX_WORKERS=4
MAX_ITERATIONS=20       # Max outer loop iterations per worker
MAX_TURNS=50           # Max turns per Claude session
SYNC_INTERVAL=36       # Run sync every N iterations (3 minutes at 5s sleep)
AGENT_TYPE="system.task-worker"  # Default agent type (can be overridden with 'plan' mode)
PID_WAIT_TIMEOUT=300   # Deciseconds to wait for agent.pid (30 seconds)
FORCE_LOCK=false       # --force flag state for lock override
MAX_SKIP_RETRIES=3     # Kanban update failures before permanent skip
FIX_WORKER_TIMEOUT=1800  # Fix worker max runtime (seconds)
FIX_WORKER_LIMIT=2     # Max concurrent fix workers
RESOLVE_WORKER_TIMEOUT=1800  # Resolve worker max runtime (seconds)
AGING_FACTOR=7         # Scheduling events per priority level promotion
SIBLING_WIP_PENALTY=20000  # Fixed-point penalty when sibling is WIP (20000 = 2.0)
PLAN_BONUS=15000           # Fixed-point bonus for tasks with plans (15000 = 1.5)
DEP_BONUS_PER_TASK=7000    # Fixed-point bonus per task blocked (7000 = 0.7)

show_help() {
    cat << EOF
wiggum run - Orchestrate workers for incomplete tasks

Usage: wiggum run [mode] [options]

Modes:
  (default)            Use system.task-worker agent (standard execution)
  plan                 Use system.task-worker agent with plan mode enabled (creates
                       implementation plan before execution)

Options:
  --max-workers N      Maximum concurrent workers (default: 4)
  --max-iters N        Maximum iterations per worker (default: 20)
  --max-turns N        Maximum turns per Claude session (default: 50)
  --pipeline NAME      Pipeline config to use (from config/pipelines/ or config/)
  --force              Override stale orchestrator lock
  -h, --help          Show this help message

Examples:
  wiggum run                              # Start orchestration with defaults
  wiggum run plan                         # Use planning mode for all workers
  wiggum run --max-workers 8              # Start with max 8 workers
  wiggum run plan --max-workers 2         # Planning mode with 2 workers
  wiggum run --pipeline fast              # Use the fast pipeline

Behavior:
  - Chief assigns pending tasks [ ] to workers based on dependency graph
  - Tasks are scheduled by priority: HIGH > MEDIUM > LOW
  - Tasks with unsatisfied dependencies are blocked until deps complete
  - Tasks are marked in-progress [=] when assigned
  - Workers mark tasks pending approval [P] when PR is created
  - Periodic sync updates [P] -> [x] when PRs are merged
  - Chief waits until all tasks are complete [x]
  - New workers spawn as old ones finish (up to max)
  - Circular dependencies are detected and reported at startup

EOF
}

# Spawn a worker for a task using wiggum-start
# Sets: SPAWNED_WORKER_ID, SPAWNED_WORKER_PID (for caller to use)
spawn_worker() {
    local task_id="$1"

    # Use wiggum-start to start the worker, capturing exit code
    local start_output
    local start_exit_code
    start_output=$("$WIGGUM_HOME/bin/wiggum-start" "$task_id" \
        --max-iters "$MAX_ITERATIONS" --max-turns "$MAX_TURNS" \
        --agent-type "$AGENT_TYPE" 2>&1) || start_exit_code=$?
    start_exit_code=${start_exit_code:-0}

    # Handle specific exit codes
    if [ "$start_exit_code" -eq "$EXIT_WORKER_ALREADY_EXISTS" ]; then
        # Worker directory exists from previous run
        # Exclude plan workers (worker-TASK-xxx-plan-*) - those are read-only planning sessions
        local existing_dir
        existing_dir=$(find_any_worker_by_task_id "$RALPH_DIR" "$task_id" | grep -v -- '-plan-' || true)
        if [ -n "$existing_dir" ]; then
            # Check if the worker process is still running
            local stale_pid
            stale_pid=$(cat "$existing_dir/agent.pid" 2>/dev/null || true)
            if [ -n "$stale_pid" ] && kill -0 "$stale_pid" 2>/dev/null; then
                # Process is still running, refuse to spawn duplicate
                log_error "Worker for $task_id is still running (PID: $stale_pid)"
                log_error "Use 'wiggum stop $task_id' or 'wiggum kill $task_id' first"
                return 1
            fi
            # Process not running - check if it's resumable
            if ! _is_terminal_failure "$existing_dir"; then
                # Worker is resumable - let resume logic handle it
                log "Worker for $task_id is resumable, skipping spawn"
                return 1
            fi
            # Terminal failure - clean up and retry fresh
            log "Cleaning up terminal-failure worker for $task_id: $(basename "$existing_dir")"
            rm -rf "$existing_dir"
            # Retry spawning - reset exit code first
            start_exit_code=0
            start_output=$("$WIGGUM_HOME/bin/wiggum-start" "$task_id" \
                --max-iters "$MAX_ITERATIONS" --max-turns "$MAX_TURNS" \
                --agent-type "$AGENT_TYPE" 2>&1) || start_exit_code=$?
        fi
    fi

    # Check if spawn succeeded
    if [ "$start_exit_code" -ne 0 ]; then
        log_error "wiggum start failed (exit $start_exit_code): $start_output"
        return 1
    fi

    # Find the worker directory that was just created (using shared library)
    local worker_dir
    worker_dir=$(find_worker_by_task_id "$RALPH_DIR" "$task_id")

    if [ -z "$worker_dir" ]; then
        log_error "Failed to find worker directory for $task_id"
        return 1
    fi

    SPAWNED_WORKER_ID=$(basename "$worker_dir")

    # Wait for agent.pid to appear (using shared library)
    if ! wait_for_worker_pid "$worker_dir" "$PID_WAIT_TIMEOUT"; then
        log_error "Agent PID file not created for $task_id"
        return 1
    fi

    SPAWNED_WORKER_PID=$(cat "$worker_dir/agent.pid")
    activity_log "worker.spawned" "$SPAWNED_WORKER_ID" "$task_id" "pid=$SPAWNED_WORKER_PID"
}

# Run periodic sync to update PR statuses and detect new comments
run_periodic_sync() {
    # Call wiggum review sync and capture output
    local sync_output sync_exit=0
    sync_output=$("$WIGGUM_HOME/bin/wiggum-review" sync 2>&1) || sync_exit=$?

    if [ $sync_exit -ne 0 ]; then
        log_error "Periodic sync failed"
        echo "$sync_output" | sed 's/^/  [sync] /'
        return
    fi

    # Parse sync results - only show output if something happened
    local merged_count comments_count
    merged_count=$(echo "$sync_output" | sed -n 's/.*Merged PRs updated: \([0-9]*\).*/\1/p' | head -1)
    comments_count=$(echo "$sync_output" | sed -n 's/.*Tasks with new comments: \([0-9]*\).*/\1/p' | head -1)
    merged_count=${merged_count:-0}
    comments_count=${comments_count:-0}

    if [ "$comments_count" -gt 0 ]; then
        log "PR sync: $comments_count task(s) with new comments"
        echo "$sync_output" | sed 's/^/  [sync] /'

        # Check for tasks needing fixes
        local tasks_needing_fix="$RALPH_DIR/.tasks-needing-fix.txt"
        if [ -s "$tasks_needing_fix" ]; then
            log "Tasks need comment fixes - will spawn fix workers"
        fi
    fi
}

# Pre-worker checks before spawning a new worker
# Returns 0 if safe to proceed, 1 if conflicts detected
pre_worker_checks() {
    # Pull latest changes from main with retry
    log "Pulling latest changes from origin/main..."

    local pull_output
    local max_attempts=3
    local delays=(2 4)

    for ((attempt=1; attempt<=max_attempts; attempt++)); do
        if pull_output=$(git pull --ff-only origin main 2>&1); then
            break
        fi

        # Immediately fail on conflicts (non-transient)
        if echo "$pull_output" | grep -qi "CONFLICT"; then
            log_error "Git pull conflict detected: $pull_output"
            log_error "Cannot spawn new workers with unresolved conflicts"
            return 1
        fi

        # On last attempt, give up
        if [ $attempt -eq $max_attempts ]; then
            log_error "Git pull failed after $max_attempts attempts: $pull_output"
            return 1
        fi

        # Transient error - retry with backoff
        local delay=${delays[$((attempt-1))]}
        log "Git pull attempt $attempt failed (transient), retrying in ${delay}s..."
        sleep "$delay"
    done

    # Check for conflicts with active worktrees
    local workers_dir="$RALPH_DIR/workers"
    if [ -d "$workers_dir" ]; then
        for worker_dir in "$workers_dir"/worker-*; do
            [ -d "$worker_dir/workspace" ] || continue

            local workspace="$worker_dir/workspace"
            if [ -d "$workspace/.git" ] || [ -f "$workspace/.git" ]; then
                # Check if worktree has conflicts with main
                if git -C "$workspace" diff --name-only origin/main 2>/dev/null | \
                   xargs -I {} git -C "$workspace" diff --check origin/main -- {} 2>&1 | \
                   grep -q "conflict"; then
                    log_error "Conflict detected in $(basename "$worker_dir")"
                    return 1
                fi
            fi
        done
    fi

    return 0
}

# Handle main worker completion (callback for pool_cleanup_finished)
_handle_main_worker_completion() {
    local worker_dir="$1"
    local task_id="$2"
    activity_log "worker.completed" "" "$task_id" "worker_dir=$worker_dir"
    log "Worker for $task_id finished"
    scheduler_mark_event
}

# Handle fix worker completion (callback for pool_cleanup_finished)
_handle_fix_worker_completion() {
    local worker_dir="$1"
    local task_id="$2"

    if handle_fix_worker_completion "$worker_dir" "$task_id"; then
        # Fix succeeded - attempt merge if needed
        if git_state_is "$worker_dir" "needs_merge"; then
            attempt_pr_merge "$worker_dir" "$task_id" "$RALPH_DIR" || true
        fi
    fi
}

# Handle fix worker timeout (callback for pool_cleanup_finished)
_handle_fix_worker_timeout() {
    local worker_dir="$1"
    local task_id="$2"
    handle_fix_worker_timeout "$worker_dir" "$task_id" "$FIX_WORKER_TIMEOUT"
}

# Handle resolve worker completion (callback for pool_cleanup_finished)
_handle_resolve_worker_completion() {
    local worker_dir="$1"
    local task_id="$2"

    if handle_resolve_worker_completion "$worker_dir" "$task_id"; then
        # Resolution succeeded - attempt merge if needed
        if git_state_is "$worker_dir" "needs_merge"; then
            attempt_pr_merge "$worker_dir" "$task_id" "$RALPH_DIR" || true
        fi
    fi
}

# Handle resolve worker timeout (callback for pool_cleanup_finished)
_handle_resolve_worker_timeout() {
    local worker_dir="$1"
    local task_id="$2"
    handle_resolve_worker_timeout "$worker_dir" "$task_id" "$RESOLVE_WORKER_TIMEOUT"
}

# Resume a stopped worker directly (without LLM analysis)
#
# Uses the current step from pipeline-config.json to resume execution.
# This is faster than `wiggum resume` which uses an LLM to analyze logs.
#
# Args:
#   worker_dir   - Worker directory path
#   task_id      - Task identifier
#   resume_step  - Step ID to resume from
_resume_worker_direct() {
    local worker_dir="$1"
    local task_id="$2"
    local resume_step="$3"

    log "Resuming $task_id from step '$resume_step'"

    # Launch system.task-worker from resume step
    export _WORKER_DIR="$worker_dir"
    export _WORKER_PROJECT_DIR="$PROJECT_DIR"
    export _WORKER_MAX_ITERATIONS="$MAX_ITERATIONS"
    export _WORKER_MAX_TURNS="$MAX_TURNS"
    export _WORKER_RESUME_STEP="$resume_step"
    export _WORKER_WIGGUM_HOME="$WIGGUM_HOME"

    # shellcheck disable=SC2016
    setsid bash -c '
        set -euo pipefail
        export WIGGUM_HOME="$_WORKER_WIGGUM_HOME"
        source "$WIGGUM_HOME/lib/worker/agent-registry.sh"
        run_agent "system.task-worker" "$_WORKER_DIR" "$_WORKER_PROJECT_DIR" 30 \
            "$_WORKER_MAX_ITERATIONS" "$_WORKER_MAX_TURNS" "$_WORKER_RESUME_STEP" ""
    ' >> "$RALPH_DIR/logs/workers.log" 2>&1 &
}

# Schedule resume of stopped workers
#
# Called once at startup to resume any stopped workers before starting new
# tasks. Workers are resumed from their current pipeline step.
#
# Skips workers that:
#   - Are terminal failures (last step + FAIL)
#   - Are at capacity (MAX_WORKERS)
#   - Have tasks no longer pending/in-progress
_schedule_resume_workers() {
    local resumable
    resumable=$(get_resumable_workers "$RALPH_DIR")
    [ -n "$resumable" ] || return 0

    log "Checking for stopped workers to resume..."

    while read -r worker_dir task_id current_step; do
        [ -n "$worker_dir" ] || continue

        # Check capacity
        local main_count
        main_count=$(pool_count "main")
        if [ "$main_count" -ge "$MAX_WORKERS" ]; then
            log "At capacity ($main_count/$MAX_WORKERS) - deferring remaining resumes"
            break
        fi

        # Check task is still pending or in-progress
        local task_status
        task_status=$(get_task_status "$RALPH_DIR/kanban.md" "$task_id")
        case "$task_status" in
            " "|"=") ;;
            *)
                log "Skipping resume of $task_id - task status is '$task_status'"
                continue
                ;;
        esac

        # Mark as in-progress if pending
        if [ "$task_status" = " " ]; then
            if ! update_kanban_status "$RALPH_DIR/kanban.md" "$task_id" "="; then
                log_error "Failed to mark $task_id as in-progress"
                continue
            fi
        fi

        # Resume the worker
        _resume_worker_direct "$worker_dir" "$task_id" "$current_step"

        # Track the worker
        if wait_for_worker_pid "$worker_dir" "$PID_WAIT_TIMEOUT"; then
            local pid
            pid=$(cat "$worker_dir/agent.pid")
            pool_add "$pid" "main" "$task_id"
            scheduler_mark_event
            activity_log "worker.resumed" "$(basename "$worker_dir")" "$task_id" "step=$current_step pid=$pid"
            log "Resumed worker for $task_id (PID: $pid) at step '$current_step'"
        else
            log_error "Resume started but PID not created for $task_id"
        fi
    done <<< "$resumable"
}

main() {
    # Parse run options
    while [[ $# -gt 0 ]]; do
        case "$1" in
            plan)
                export WIGGUM_PLAN_MODE=true
                shift
                ;;
            --max-workers)
                if [[ -z "${2:-}" ]] || [[ "${2:-}" =~ ^- ]]; then
                    echo "Error: --max-workers requires a number argument"
                    exit $EXIT_USAGE
                fi
                MAX_WORKERS="$2"
                shift 2
                ;;
            --max-iters)
                if [[ -z "${2:-}" ]] || [[ "${2:-}" =~ ^- ]]; then
                    echo "Error: --max-iters requires a number argument"
                    exit $EXIT_USAGE
                fi
                MAX_ITERATIONS="$2"
                shift 2
                ;;
            --max-turns)
                if [[ -z "${2:-}" ]] || [[ "${2:-}" =~ ^- ]]; then
                    echo "Error: --max-turns requires a number argument"
                    exit $EXIT_USAGE
                fi
                MAX_TURNS="$2"
                shift 2
                ;;
            --pipeline)
                if [[ -z "${2:-}" ]] || [[ "${2:-}" =~ ^- ]]; then
                    echo "Error: --pipeline requires a name argument"
                    exit $EXIT_USAGE
                fi
                export WIGGUM_PIPELINE="$2"
                shift 2
                ;;
            --force)
                FORCE_LOCK=true
                shift
                ;;
            -h|--help)
                show_help
                exit $EXIT_OK
                ;;
            -*)
                echo "Unknown option: $1"
                echo ""
                show_help
                exit $EXIT_USAGE
                ;;
            *)
                echo "Unknown argument: $1"
                echo ""
                show_help
                exit $EXIT_USAGE
                ;;
        esac
    done

    # Initialize project if needed
    if [ ! -d "$RALPH_DIR" ]; then
        log_error ".ralph/ directory not found. Run 'wiggum init' first."
        exit $EXIT_RUN_NO_RALPH_DIR
    fi

    # Load rate limit configuration
    load_rate_limit_config

    # Initialize activity log
    activity_init "$PROJECT_DIR"

    # Ensure only one orchestrator runs at a time (not local - needed by trap handler)
    orchestrator_lock="$RALPH_DIR/.orchestrator.pid"

    # Check if another orchestrator is already running
    if [ -f "$orchestrator_lock" ]; then
        local existing_pid
        existing_pid=$(cat "$orchestrator_lock" 2>/dev/null)

        # Validate PID is a number
        if [[ "$existing_pid" =~ ^[0-9]+$ ]]; then
            # Check if that process is still running and is wiggum-run
            if kill -0 "$existing_pid" 2>/dev/null; then
                if ps -p "$existing_pid" -o args= 2>/dev/null | grep -q "wiggum-run"; then
                    if [ "$FORCE_LOCK" = true ]; then
                        log "WARNING: Overriding lock held by running orchestrator (PID: $existing_pid) due to --force"
                        rm -f "$orchestrator_lock"
                    else
                        log_error "Another wiggum-run orchestrator is already running (PID: $existing_pid)"
                        echo ""
                        echo "Only one orchestrator can run at a time to prevent conflicts."
                        echo "If you're sure no orchestrator is running, remove: $orchestrator_lock"
                        echo "Or use --force to override the lock."
                        exit $EXIT_RUN_ORCHESTRATOR_RUNNING
                    fi
                else
                    # PID exists but it's not wiggum-run (PID reused)
                    log "Cleaning stale orchestrator lock (PID reused)"
                    rm -f "$orchestrator_lock"
                fi
            else
                # Process no longer running
                log "Cleaning stale orchestrator lock"
                rm -f "$orchestrator_lock"
            fi
        else
            # Invalid PID in lock file
            log "Cleaning invalid orchestrator lock"
            rm -f "$orchestrator_lock"
        fi
    fi

    # Create orchestrator lock file
    echo "$$" > "$orchestrator_lock"
    log "Created orchestrator lock (PID: $$)"

    # Track shutdown state (not local - needed by trap handlers)
    _ORCH_SHUTDOWN_REQUESTED=false

    # Setup trap to cleanup lock file on exit
    cleanup_orchestrator() {
        if [ "${_ORCH_SHUTDOWN_REQUESTED:-false}" = false ]; then
            log "Cleaning up orchestrator lock"
            _ORCH_SHUTDOWN_REQUESTED=true
            rm -f "$orchestrator_lock"
        fi
    }
    trap cleanup_orchestrator EXIT

    # Handle INT and TERM signals - stop orchestration but leave workers running
    handle_shutdown_signal() {
        log ""
        log "Shutdown signal received - stopping orchestrator"
        log "Active workers will continue running to completion"
        log "Use 'wiggum status' to monitor worker progress"
        cleanup_orchestrator
        exit $EXIT_SIGINT
    }
    trap handle_shutdown_signal INT TERM

    if [ ! -f "$RALPH_DIR/kanban.md" ]; then
        log_error ".ralph/kanban.md not found. Create a kanban file first."
        exit $EXIT_RUN_NO_KANBAN
    fi

    # Validate kanban format before running
    log "Validating kanban.md format..."
    if ! "$WIGGUM_HOME/bin/wiggum-validate" --quiet; then
        log_error "Kanban validation failed. Run 'wiggum validate' to see details."
        exit $EXIT_RUN_VALIDATION_FAILED
    fi
    log "Kanban validation passed"

    # Initialize scheduler
    scheduler_init "$RALPH_DIR" "$PROJECT_DIR" "$AGING_FACTOR" "$SIBLING_WIP_PENALTY" "$PLAN_BONUS" "$DEP_BONUS_PER_TASK"

    # Check for self and circular dependencies
    log "Checking for dependency cycles..."
    scheduler_detect_cycles || true

    # Check for clean git status
    if [ -n "$(git status --porcelain 2>/dev/null)" ]; then
        log_error "Git working directory is not clean. Please commit or stash your changes before running."
        echo ""
        echo "Uncommitted changes detected:"
        git status --short
        exit $EXIT_RUN_GIT_DIRTY
    fi

    # Pre-flight checks: Ensure SSH keys are cached and gh is authenticated
    log "Running pre-flight checks..."

    # Extract hostname from git remote
    local git_remote
    git_remote=$(git remote get-url origin 2>/dev/null)
    if [ -n "$git_remote" ]; then
        # Extract hostname from SSH URLs (git@github.com:user/repo.git or ssh://git@github.com/user/repo.git)
        local git_host=""
        if [[ "$git_remote" =~ ^git@([^:]+): ]]; then
            git_host="${BASH_REMATCH[1]}"
        elif [[ "$git_remote" =~ ^ssh://git@([^/]+)/ ]]; then
            git_host="${BASH_REMATCH[1]}"
        fi

        if [ -n "$git_host" ]; then
            echo "  → Testing SSH connection to $git_host..."
            local ssh_output
            ssh_output=$(ssh -T "git@$git_host" 2>&1) || true
            echo "$ssh_output" | head -5
            if ! echo "$ssh_output" | grep -qi "successfully authenticated"; then
                log_error "SSH test failed. Please ensure your SSH keys are set up and the agent is running."
                echo ""
                echo "Try running: ssh -T git@$git_host"
                exit $EXIT_RUN_SSH_FAILED
            fi
            echo "  ✓ SSH connection successful"
        fi
    fi

    # Test GitHub CLI authentication
    echo "  → Checking gh auth status..."
    if gh auth status &>/dev/null; then
        echo "  ✓ GitHub CLI authenticated"
    else
        log_error "gh auth check failed. Please log in with: gh auth login"
        echo ""
        echo "Try running: gh auth status"
        exit $EXIT_RUN_GH_AUTH_FAILED
    fi

    echo ""

    # Check for failed tasks and reset them to pending for retry
    local failed_tasks
    failed_tasks=$(get_failed_tasks "$RALPH_DIR/kanban.md")
    if [ -n "$failed_tasks" ]; then
        log "Found failed tasks - resetting for retry:"
        for task_id in $failed_tasks; do
            echo "  → Retrying $task_id"
            if ! update_kanban_status "$RALPH_DIR/kanban.md" "$task_id" " "; then
                log_error "Failed to reset $task_id to pending"
            fi
        done
        echo ""
    fi

    local mode_desc="standard"
    if [ "${WIGGUM_PLAN_MODE:-false}" = "true" ]; then
        mode_desc="plan mode"
    fi
    if [ -n "${WIGGUM_PIPELINE:-}" ]; then
        mode_desc="$mode_desc, pipeline: $WIGGUM_PIPELINE"
    fi
    log "Starting Chief Wiggum in $PROJECT_DIR ($mode_desc, max $MAX_WORKERS concurrent workers)"
    echo ""
    echo "Press Ctrl+C to stop and view 'wiggum status' for details"
    echo "=========================================="
    echo ""

    # Restore active workers from existing worker directories
    scheduler_restore_workers

    # Resume any stopped workers before starting new ones
    _schedule_resume_workers

    local iteration=0
    local sync_counter=0

    # Main orchestration loop
    while true; do
        ((++iteration))
        ((++sync_counter))

        # Run periodic sync every SYNC_INTERVAL iterations
        if [ $sync_counter -ge $SYNC_INTERVAL ]; then
            run_periodic_sync
            sync_counter=0

            # Update shared usage data periodically
            usage_tracker_write_shared "$RALPH_DIR" > /dev/null 2>&1 || true

            # Decay skip counts - give tasks another chance
            scheduler_decay_skip_counts

            # Create workspaces for orphaned PRs (PRs needing fixes but no local workspace)
            create_orphan_pr_workspaces "$RALPH_DIR" "$PROJECT_DIR"

            # Check for and spawn fix workers after sync
            spawn_fix_workers "$RALPH_DIR" "$PROJECT_DIR" "$FIX_WORKER_LIMIT"
        fi

        # Tick scheduler to update task lists
        scheduler_tick

        # Clean up finished main workers
        pool_cleanup_finished "main" 0 _handle_main_worker_completion ""

        # Detect orphan workers not in our tracking
        scheduler_detect_orphan_workers

        # Clean up finished/timed-out fix workers
        pool_cleanup_finished "fix" "$FIX_WORKER_TIMEOUT" _handle_fix_worker_completion _handle_fix_worker_timeout

        # Clean up finished/timed-out resolve workers
        pool_cleanup_finished "resolve" "$RESOLVE_WORKER_TIMEOUT" _handle_resolve_worker_completion _handle_resolve_worker_timeout

        # Check for workers needing conflict resolution and spawn resolvers
        spawn_resolve_workers "$RALPH_DIR" "$PROJECT_DIR" "$FIX_WORKER_LIMIT"

        # Check if we're done (no pending tasks and no active workers)
        if scheduler_is_complete; then
            display_final_summary "$RALPH_DIR"
            break
        fi

        # Check rate limit before spawning
        if rate_limit_check "$RALPH_DIR"; then
            local cycle_prompts
            cycle_prompts=$(jq -r '.current_5h_cycle.total_prompts // 0' "$RALPH_DIR/claude-usage.json" 2>/dev/null)
            log "Rate limit threshold reached ($cycle_prompts >= $WIGGUM_RATE_LIMIT_THRESHOLD prompts in 5h cycle)"
            activity_log "rate_limit.detected" "" "" "prompts=$cycle_prompts threshold=$WIGGUM_RATE_LIMIT_THRESHOLD"
            rate_limit_wait_for_cycle_reset
            activity_log "rate_limit.resumed" "" ""
            usage_tracker_write_shared "$RALPH_DIR" > /dev/null 2>&1 || true
            continue  # Re-check state after reset
        fi

        # Spawn workers for ready tasks (up to MAX_WORKERS limit)
        for task_id in $SCHED_READY_TASKS; do
            # Check if we can spawn this task
            if ! scheduler_can_spawn_task "$task_id" "$MAX_WORKERS"; then
                case "$SCHED_SKIP_REASON" in
                    at_capacity) break ;;  # Stop trying when at capacity
                    file_conflict)
                        # Build conflict info for logging
                        local -A _temp_workers=()
                        _build_workers() {
                            local pid="$1" type="$2" tid="$3"
                            [ "$type" = "main" ] && _temp_workers[$pid]="$tid"
                        }
                        pool_foreach "main" _build_workers
                        local conflicts
                        conflicts=$(get_conflicting_files "$RALPH_DIR" "$task_id" _temp_workers | tr '\n' ',' | sed 's/,$//')
                        log "Deferring $task_id - file conflict: $conflicts"
                        ;;
                    *) ;;  # Skip silently for cyclic_dependency, skip_count
                esac
                continue
            fi

            # Run pre-worker checks (git pull, conflict detection)
            if ! pre_worker_checks; then
                log_error "Pre-worker checks failed for $task_id - skipping this task"
                continue
            fi

            # Get task priority for logging
            local task_priority
            task_priority=$(get_task_priority "$RALPH_DIR/kanban.md" "$task_id")

            # Mark task as in-progress in kanban
            log "Assigning $task_id (Priority: ${task_priority:-MEDIUM}) to new worker"
            if ! update_kanban_status "$RALPH_DIR/kanban.md" "$task_id" "="; then
                log_error "Failed to mark $task_id as in-progress"
                scheduler_increment_skip "$task_id"
                local skip_count
                skip_count=$(scheduler_get_skip_count "$task_id")
                log "Task $task_id skip count: $skip_count/$MAX_SKIP_RETRIES"
                if [ "$skip_count" -ge "$MAX_SKIP_RETRIES" ]; then
                    log_error "Task $task_id permanently skipped after $MAX_SKIP_RETRIES consecutive kanban failures"
                fi
                continue
            fi

            # Spawn worker (wiggum-start handles backgrounding)
            if ! spawn_worker "$task_id"; then
                log_error "Failed to spawn worker for $task_id"
                update_kanban_status "$RALPH_DIR/kanban.md" "$task_id" "*"
                continue
            fi

            pool_add "$SPAWNED_WORKER_PID" "main" "$task_id"
            scheduler_mark_event

            # Remove from ready-since tracking (task is no longer waiting)
            scheduler_remove_from_aging "$task_id"

            # Log task assignment to audit log
            audit_log_task_assigned "$task_id" "$SPAWNED_WORKER_ID" "$SPAWNED_WORKER_PID"

            log "Spawned worker $SPAWNED_WORKER_ID for $task_id (PID: $SPAWNED_WORKER_PID)"
        done

        # Update aging tracking when scheduling events occurred
        scheduler_update_aging

        # Show status only when scheduling events occur
        if [ "$SCHED_SCHEDULING_EVENT" = true ]; then
            local cyclic_ref
            cyclic_ref=$(scheduler_get_cyclic_tasks_ref)
            display_orchestrator_status \
                "$iteration" \
                "$MAX_WORKERS" \
                "$SCHED_READY_TASKS" \
                "$SCHED_BLOCKED_TASKS" \
                "$cyclic_ref" \
                "$RALPH_DIR" \
                "$(scheduler_get_ready_since_file)" \
                "$AGING_FACTOR" \
                "$PLAN_BONUS" \
                "$DEP_BONUS_PER_TASK"
        fi

        # Wait a bit before checking again
        sleep 5
    done
}

main "$@"
